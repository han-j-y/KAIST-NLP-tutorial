{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "If3NrtT6cW26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This tutorial is based on the following tutorial\n",
        "# Webpage: https://towardsdatascience.com/visualizing-elmo-contextual-vectors-94168768fdaa\n",
        "# Git-repo: https://github.com/hengluchang/visualizing_contextual_vectors/blob/master/elmo_vis.py\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2iKnVYODw-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEEw1dQghQvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Elmo:\n",
        "    def __init__(self):\n",
        "        self.elmo = ElmoEmbedder()\n",
        "\n",
        "    # Append each word vector into a matrix\n",
        "    def get_elmo_vector(self, tokens, layer):\n",
        "        vectors = self.elmo.embed_sentence(tokens)\n",
        "        X = []\n",
        "        for vector in vectors[layer]:\n",
        "            X.append(vector)\n",
        "\n",
        "        X = np.array(X)\n",
        "\n",
        "        return X\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pag30X5AEIML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from allennlp.commands.elmo import ElmoEmbedder\n",
        "elmo = ElmoEmbedder()\n",
        "tokens = [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"]\n",
        "vectors = elmo.embed_sentence(tokens)\n",
        "\n",
        "assert(len(vectors) == 3) # one for each layer in the ELMo output\n",
        "assert(len(vectors[0]) == len(tokens)) # the vector elements correspond with the input tokens\n",
        "\n",
        "import scipy\n",
        "vectors2 = elmo.embed_sentence([\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"])\n",
        "#vectors3 = elmo.embed_sentence([\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"])\n",
        "\n",
        "\n",
        "print(scipy.spatial.distance.cosine(vectors[2][3], vectors2[2][3])) # cosine distance between \"apple\" and \"carrot\" in the last layer\n",
        "print(scipy.spatial.distance.cosine(vectors[2][1], vectors2[2][1])) # cosine distance between \"ate\" and \"ate\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfISU_gfHHcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def dim_reduction(X, n):\n",
        "    pca = PCA(n_components=n)\n",
        "    print(\"size of X: {}\".format(X.shape))\n",
        "    results = pca.fit_transform(X)\n",
        "    print(\"size of reduced X: {}\".format(results.shape))\n",
        "\n",
        "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "        print(\"Variance retained ratio of PCA-{}: {}\".format(i+1, ratio))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot(word, token_list, reduced_X, file_name, title):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot ELMo vectors\n",
        "    i = 0\n",
        "    for j, token in enumerate(token_list):\n",
        "        color = pick_color(j)\n",
        "        for _, w in enumerate(token):\n",
        "\n",
        "            # only plot the word of interest\n",
        "            if w.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
        "                ax.plot(reduced_X[i, 0], reduced_X[i, 1], color)\n",
        "            i += 1\n",
        "\n",
        "    tokens = []\n",
        "    for token in token_list:\n",
        "        tokens += token\n",
        "\n",
        "    # annotate point\n",
        "    k = 0\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
        "            text = ' '.join(token_list[k])\n",
        "\n",
        "            # bold the word of interest in the sentence\n",
        "            text = text.replace(token, r\"$\\bf{\" + token + \"}$\")\n",
        "\n",
        "            plt.annotate(text, xy=(reduced_X[i, 0], reduced_X[i, 1]))\n",
        "            k += 1\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"PCA 1\")\n",
        "    ax.set_ylabel(\"PCA 2\")\n",
        "    fig.savefig(file_name, bbox_inches=\"tight\")\n",
        "\n",
        "    print(\"{} saved\\n\".format(file_name))\n",
        "\n",
        "\n",
        "def pick_color(i):\n",
        "    if i == 0:\n",
        "        color = 'ro'\n",
        "    elif i == 1:\n",
        "        color = 'bo'\n",
        "    elif i == 2:\n",
        "        color = 'yo'\n",
        "    elif i == 3:\n",
        "        color = 'go'\n",
        "    else:\n",
        "        color = 'co'\n",
        "    return color\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXGddOOxhSXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Elmo()\n",
        "\n",
        "banks = OrderedDict()\n",
        "banks[0] = \"One can deposit money at the bank\"\n",
        "banks[1] = \"He had a nice walk along the river bank\"\n",
        "banks[2] = \"I withdrew cash from the bank\"\n",
        "banks[3] = \"The river bank was not clean\"\n",
        "banks[4] = \"My wife and I have a joint bank account\"\n",
        "\n",
        "cans = OrderedDict()\n",
        "cans[0] = \"One can deposit money at the bank\"\n",
        "cans[1] = \"I have a can of coke\"\n",
        "cans[2] = \"I can take care of myself\"\n",
        "cans[3] = \"You'll need a can of tuna for this recipe.\"\n",
        "cans[4] = \"He works in a factory where they can fruit.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# works = OrderedDict()\n",
        "# works[0] = \"I like this beautiful work by Andy Warhol\"\n",
        "# works[1] = \"Employee works hard every day\"\n",
        "# works[2] = \"My sister works at Starbucks\"\n",
        "# works[3] = \"This amazing work was done in the early nineteenth century\"\n",
        "# works[4] = \"Hundreds of people work in this building\"\n",
        "\n",
        "# plants = OrderedDict()\n",
        "# plants[0] = \"The gardener planted some trees in my yard\"\n",
        "# plants[1] = \"I plan to plant a Joshua tree tomorrow\"\n",
        "# plants[2] = \"My sister planted a seed and hopes it will grow to a tree\"\n",
        "# plants[3] = \"This kind of plant only grows in the subtropical region\"\n",
        "# plants[4] = \"Most of the plants will die without water\"\n",
        "\n",
        "# words = {\n",
        "# \"bank\": banks,\n",
        "# \"work\": works,\n",
        "# \"plant\": plants\n",
        "# }\n",
        "\n",
        "# contextual vectors for ELMo layer 1 and 2\n",
        "#for layer in [1, 2]:\n",
        "layer = 1 \n",
        "  #for word, sentences in words.items():\n",
        "word = 'can'\n",
        "sentences = cans\n",
        "print(\"visualizing word {} using ELMo layer {}\".format(word, layer))\n",
        "X = np.concatenate([model.get_elmo_vector(tokens=sentences[idx].split(),\n",
        "                                          layer=layer)\n",
        "                    for idx, _ in enumerate(sentences)], axis=0)\n",
        "\n",
        "# The first 2 principal components\n",
        "X_reduce = dim_reduction(X=X, n=2)\n",
        "\n",
        "token_list = [] #[[w11,...,w1n], [w21,...,w2n],...,[w51,...,w5n]]\n",
        "for _, sentence in sentences.items():\n",
        "    token_list.append(sentence.split())\n",
        "\n",
        "file_name = \"{}_elmo_layer_{}.png\".format(word, layer)\n",
        "title = \"Layer {} ELMo vectors of the word {}\".format(layer, word)\n",
        "#plot(word, token_list, X_reduce, file_name, title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akvTHwRvsazi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(token_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtRaIAPOsOkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# plot ELMo vectors\n",
        "i = 0 #token_id\n",
        "for j, token in enumerate(token_list):\n",
        "    color = pick_color(j)\n",
        "    for _, w in enumerate(token):\n",
        "\n",
        "        # only plot the word of interest\n",
        "        if w.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
        "            ax.plot(X_reduce[i, 0], X_reduce[i, 1], color)\n",
        "        i += 1\n",
        "\n",
        "tokens = [] #flatten token_list\n",
        "for token in token_list:\n",
        "    tokens += token\n",
        "    \n",
        "    \n",
        "# annotate point\n",
        "k = 0\n",
        "for i, token in enumerate(tokens):\n",
        "    if token.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
        "        text = ' '.join(token_list[k])\n",
        "\n",
        "        # bold the word of interest in the sentence\n",
        "        text = text.replace(token, r\"$\\bf{\" + token + \"}$\")\n",
        "\n",
        "        plt.annotate(text, xy=(X_reduce[i, 0], X_reduce[i, 1]))\n",
        "        k += 1\n",
        "\n",
        "ax.set_title(title)\n",
        "ax.set_xlabel(\"PCA 1\")\n",
        "ax.set_ylabel(\"PCA 2\")\n",
        "#fig.savefig(file_name, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXdD9Sf_tcYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in [1, 2]:\n",
        "        #for word, sentences in words.items():\n",
        "        print(\"visualizing word {} using ELMo layer {}\".format(word, layer))\n",
        "        X = np.concatenate([model.get_elmo_vector(tokens=sentences[idx].split(),\n",
        "                                                  layer=layer)\n",
        "                            for idx, _ in enumerate(sentences)], axis=0)\n",
        "\n",
        "        # The first 2 principal components\n",
        "        X_reduce = dim_reduction(X=X, n=2)\n",
        "\n",
        "        token_list = []\n",
        "        for _, sentence in sentences.items():\n",
        "            token_list.append(sentence.split())\n",
        "\n",
        "        file_name = \"{}_elmo_layer_{}.png\".format(word, layer)\n",
        "        title = \"Layer {} ELMo vectors of the word {}\".format(layer, word)\n",
        "        plot(word, token_list, X_reduce, file_name, title)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}